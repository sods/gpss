{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From raw data to meaningful features\n",
    "### Andreas Damianou\n",
    "***For the Summer School in Data Science and Machine Learning, \n",
    "Makerere University, Uganda, 27 June 2016***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTRUCTIONS - README\n",
    "\n",
    "This notebook will guide you through some basic theory of working with features. Go through it sequentially, run all the code step by step. Some code cells are already complete and you just have to run them. Some other cells (denoted with **TODO**) require you to fill in the code.\n",
    "\n",
    "If you get stuck in one of the questions, don't lose too much time, ask a demonstrator for help or move on to the next question.\n",
    "\n",
    "When the code is already there, make sure you understand what it's doing, don't just run it and continue without understanding it!\n",
    "\n",
    "**!!! So, look for parts with **TODO** and make sure you fill in the correct answer to the questions.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the plotting and numerical libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with toy data for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll consider the task of extracting features for some data that are meant to be used for classification.\n",
    "\n",
    "So, before anything, let's start revising the code for K-Nearest Neighbours algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  CODE for NN:\n",
    "def knn(Y_training, labels_training, y_test, k):\n",
    "    from scipy.spatial import distance\n",
    "    from operator import itemgetter\n",
    "    \n",
    "    # distance_test_to_train[i] says what's the distance of the i-the training point to the y_test\n",
    "    distance_test_to_train = []\n",
    "    for i in range(Y_training.shape[0]):\n",
    "        distance_test_to_train.append(distance.euclidean(Y_training[i,:], y_test))\n",
    "    # The following list sorts the index of the distance list\n",
    "    sorted_dist_indexes = np.argsort(distance_test_to_train)\n",
    "    \n",
    "    # The indexes we want to return are only the top k \n",
    "    indexes = sorted_dist_indexes[0:k]\n",
    "    \n",
    "    # Test label is the najority of neighbours' labels\n",
    "    label_test = max(set(labels_training[indexes]), key=labels_training[indexes].tolist().count)\n",
    "    \n",
    "    return label_test, indexes, distance_test_to_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we implement a new functionality in our code, it is very important to test it and verify that it's doing what we think it's doing. The simplest way to do that, is to construct some fake data and run the newly constructed function of them. Since it's our own fake data, we know the correct answer that the function should give, so we can compare that to what the code returns.\n",
    "\n",
    "We'll test the knn function. For this we'll need to create some training data called for example: `my_Y_training`, associated with some labels: `my_labels_training`. Then, create some test instances `my_y_test` and test the code by calling:\n",
    "\n",
    "`knn(my_Y_training, my_labels_training, my_y_test, 2)`. \n",
    "\n",
    "The last argument is the number of neighbours, here selecfted to 2, but try different values too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create toy data\n",
    "my_Y_training = np.array([[0,1],[2,2],[10,15],[2,1],[2,15],[0,1],[8,14],[5,5],[6,2],[7,6],[4,10],[4,6]])\n",
    "my_labels_training = np.array([0,0,1,0,1,0,1,0,0,1,1,0])\n",
    "my_Y_test = np.array([[0,2], [9,12], [8,4],[6.5,6]])\n",
    "print(my_Y_training)\n",
    "\n",
    "# Create a list whose each element is the colour of each training point's class.\n",
    "# It's read or green, depending if the label is 0 or 1.\n",
    "colours=[('r' if i==0 else 'g') for i in my_labels_training]\n",
    "\n",
    "# Do a plot of the points. Since the points are two-dimensional (two columns), \n",
    "# the scatter plot is going to be a 2D plot. \n",
    "plt.scatter(my_Y_training[:,0],my_Y_training[:,1],marker='o',s=78,c=colours)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**TODO Q1:** Try different values for the number k below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: try other values too\n",
    "k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call the knn code for each test instance and get back the label. All test inferred labels are collecfted into a list.\n",
    "labels_test = [knn(my_Y_training, my_labels_training, my_Y_test[i:i+1,:],k)[0] for i in range(my_Y_test.shape[0])]\n",
    "# Repeat the scatter plot of above, just to have all points (test and training) together\n",
    "plt.scatter(my_Y_training[:,0],my_Y_training[:,1],marker='o',s=78,c=colours)\n",
    "\n",
    "colours=[('r' if i==0 else 'g') for i in labels_test]\n",
    "# Create a scatter plot where test data are denoted by 'x' and have clour green or red, according to what label they were assigned.\n",
    "plt.scatter(my_Y_test[:,0],my_Y_test[:,1],marker='x',s=78,c=colours)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO Q2**: What do you observe as you set k=1 or k=3? \n",
    "\n",
    "** Your answer goes here **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** TODO Q3***: Now you can do the same test as above but with 3 dimensional data. Fill in the code with toy data in three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Same as above but in 3 dimensions.\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# TODO-----\n",
    "my_Y_training = \n",
    "my_labels_training = \n",
    "my_Y_test = \n",
    "#-------\n",
    "\n",
    "print(my_Y_training)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "colours=[('r' if i==0 else 'g') for i in my_labels_training]\n",
    "ax.scatter(my_Y_training[:,0],my_Y_training[:,1],my_Y_training[:,2],marker='o',s=78,c=colours)\n",
    "\n",
    "k=2 # TODO: Also try with 1\n",
    "labels_test = [knn(my_Y_training, my_labels_training, my_Y_test[i:i+1,:],k)[0] for i in range(my_Y_test.shape[0])]\n",
    "print(labels_test)\n",
    "colours=[('r' if i==0 else 'g') for i in labels_test]\n",
    "ax.scatter(my_Y_test[:,0],my_Y_test[:,1],my_Y_test[:,2],marker='x',s=78,c=colours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test feature selection. For this, we'll need a dataset with many features per instance (i.e. `Y` will have many columns, here we'll try 5). We'll investigate how the results change as we consider different subsets of features to use.\n",
    "\n",
    "First, we'll create some toy data as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "my_Y_training = np.array([[0,1,0,2,6],[2,2,0,2,1],[10,8,0.0001,11,1],[2,1,0,2,12],[2,15,0.001,9,1],[0,1,0,8,11],[8,14,0.0001,10,6]])\n",
    "my_labels_training = np.array([0,0,1,0,1,0,1])\n",
    "my_Y_test = np.array([[0,2,0,3,6], [9,12,0,8,6], [8,4,0,8,6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to decide:\n",
    "1. How many features to use?\n",
    "2. Which features?\n",
    "\n",
    "There are many methods to do that. Here we'll explore some very basic ones.\n",
    "\n",
    "The simplest and more intuitive, is to select features by interacting with our data. Remember, the feature-selection procedure must be carried out by considering what **task** we have in mind (in our case, classification). Therefore, we'll first just plot our data alongside the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,sharey=True)\n",
    "\n",
    "axes[0].matshow(my_Y_training,aspect='auto')\n",
    "axes[1].matshow(my_labels_training[:,None],aspect='auto')\n",
    "axes[0].set_title('Y data')\n",
    "axes[1].set_title('Labels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO Q4**\n",
    "\n",
    "Just by looking at the data, answer:\n",
    "1. Which features look more descriptive with respect to labels?\n",
    "2. Which seem less descriptive?\n",
    "3. In general, could it be the case that we can increase descriptiveness by considering set (combination) of features rather than just picking one? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Your answer goes here **\n",
    "1. \n",
    "2. \n",
    "3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a truly large number of features to consider, it'll be diffult to rely on visualisation. However, we can still get a data summary by writing some code.\n",
    "\n",
    "Firstly, from the above plot we see that feature in column 2 is almost constant. This can be programmatically observed by computing the variance across features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(my_Y_training.var(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can't be sure that the infinitesimal variance around this feature is indeed neglectable. If we were doing unsupervised learning, i.e. we didn't have the label set with us, we'd probably reject this feature as it seems useless.\n",
    "\n",
    "However, here we have the labels, so we can verify that this feature is \"useless\" by checking if it follows a pattern similar to the labels. Because it's hard to verify this from the above matrix, we can magnify the differences in this feature by re-normalizing it. This motivates the idea of re-normalizing all features to compare them to the label vector. Re-normalization means that we'll make all features be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_Y_training_norm = np.zeros((my_Y_training.shape[0],my_Y_training.shape[1]))\n",
    "for j in range(my_Y_training.shape[1]):\n",
    "    tmp = my_Y_training[:,j]\n",
    "    my_Y_training_norm[:,j] = (tmp - tmp.min(0)) / tmp.ptp(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the normalized data and the label vector next to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,sharey=True)\n",
    "\n",
    "axes[0].matshow(my_Y_training_norm,aspect='auto')\n",
    "axes[1].matshow(my_labels_training[:,None],aspect='auto')\n",
    "axes[0].set_title('Y data')\n",
    "axes[1].set_title('Labels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that feature 2 is actually not that useless! It is slightly larger for data (rows of Y) that correspond to label 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TODO Q5***\n",
    "\n",
    "Let's automate the above discussion. We'll again follow a very simple approach which returns the distance of each normalized feature vector to the label vector, the intuition being that if the distance is small, then that might be a good feature.\n",
    "\n",
    "*Hint*: check the knn code to see how the distances are taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we've discussed that usually combination of features (rather than just selecting one feature) is expected to give the best results, most of the code we wrote above focuses on looking at features individually.\n",
    "\n",
    "Finding combination of features is more difficult, as the problem of considering all possible combinations makes the search space larger. We won't explore state-of-the-art methods here, but instead we'll just get a feeling as to what could be a good feature combination for our data by trying several subsets of features and measuring the classification accuracy on the test set (which in this case is rather a validation set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TODO Q6:*** \n",
    "1. Make the data so that you use only 1 feature.\n",
    "2. Plot the 1-dimesional data with different colors according to the labels and then for the test point y*\n",
    "3. find the k-nns and infer its label. What k will you use here? Why?\n",
    "4. Print the distances. \n",
    "5. Do this for different selection of feature. Which is best?\n",
    "\n",
    "*** TODO Q7:***  \n",
    "6. Now select two features (any combination you want) for your data and repeat as above.\n",
    "7. Repeat, this time by selecting 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*** TODO Q8:***\n",
    "1. Which combination of features yields better results in terms of classification accuracy? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** YOU ANSWER GOES HERE **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** TODO: Q9:**  What happens to the distances as you add features?\n",
    "\n",
    "** Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move away from toy data. We'll now look at more interesting data, namely text data. \n",
    "We'll create some fake texts. Each text is associated with a label which is 1 if the text is spam email, otherwise it has label 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = dict()\n",
    "# A vector so that labels[i] is 1 if the text is spam otherwise 0.\n",
    "labels = [1,1,0,0,1,1,0]\n",
    "\n",
    "text['0'] = ['this','email','is','a','spam','email']\n",
    "text['1'] = ['spam','spam','spamity','spam']\n",
    "text['2'] = ['this','email','is','about','python','and','python','example']\n",
    "text['3'] = ['python','is','good','for','you','python','good']\n",
    "text['4'] = ['you','should','not','learn','python']\n",
    "text['5'] = ['this','email','is','bad','email','and','you','should','not']\n",
    "text['6'] = ['this','is','a','good','email','good','as','it','can','get']\n",
    "\n",
    "num_texts = len(text.keys())\n",
    "\n",
    "# Concatenate all texts into a single big text. This will be helpful later.\n",
    "texts = []\n",
    "for tt in text.keys():\n",
    "    texts += text[tt]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the unique words in all texts by looking at the unique elements of the big concatenated text.\n",
    "uniques = np.unique(texts)\n",
    "print(uniques)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of all the unique words appearing in our text collection.\n",
    "\n",
    "Many of the words are very simple, like 'a', 'I' etc. We'd like to get rid of them. Why? Because we assume that they're so simple and versatile that they can't offer much statistical insight into the nature of the text.\n",
    "\n",
    "This process is called stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the dictionary of words that you wish to ignore from your texts\n",
    "stemming_list = ['a','the','for','and','is','I','you','he','she','it','they']\n",
    "# Helper variable, just get the unique words and transform them to list\n",
    "uniques_list = uniques.tolist()\n",
    "for i in range(len(stemming_list)): \n",
    "    try:\n",
    "        # Remove the i-th word if it is in the stemming list\n",
    "        uniques_list.remove(stemming_list[i])\n",
    "    except ValueError:\n",
    "        pass\n",
    "# Now turn the list of uniques (stemmed) back into an array\n",
    "uniques = np.array(uniques_list)\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check: Print the first text\n",
    "tmp = text['0']\n",
    "print(tmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** TODO Q10 ***:\n",
    "\n",
    "Now the important bit. We have texts as collection of words, cleaned and stemmed.\n",
    "\n",
    "How do we turn this into useful features?\n",
    "\n",
    "Think about your answer before scrolling down (don't cheat!!)\n",
    "Write your answer below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Your answer goes here ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** SCROLL DOWN, there's more!!! ........ ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** SCROLL DOWN, there's more!!! ........ ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features from Text: Creating a histogram - Bag of Words\n",
    "\n",
    "Here's one possible answer to Q10: We'll create features from text, by  making a histogram in a manner which is called a **Bag of Words**.\n",
    "\n",
    "A Bag of Words means that you'll convert each text into a **fixed sized** vector $[w_1, w_2, ... w_d]$, where $d$ is the total number of different words you may encounter, and $w_i$ tells us how many times this particular word appeared in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_hist = np.zeros((num_texts,len(uniques)))\n",
    "for i in range(num_texts):\n",
    "    for j in range(len(uniques)):\n",
    "        for k in range(len(text[str(i)])):\n",
    "            if text[str(i)][k] == uniques[j]:\n",
    "                text_hist[i,j]+=1\n",
    "print(text_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** TODO Q11 ***: \n",
    "\n",
    "1. Explain in your own words what is the Bag of Words representation.\n",
    "2. What are the limitations of this representation? What are the advantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Your answer goes here ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features appear only once, e.g. those in the first four columns. \n",
    "We can't do much statistical inference if these features appear only in one instance. Hence, we'll remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inds = np.where(text_hist.sum(0) > 1)[0]\n",
    "text_hist = text_hist[:,inds]\n",
    "uniques = uniques[inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll plot the (remaining) features alongside the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,sharey=True)\n",
    "axes[0].matshow(text_hist, interpolation='none',aspect='auto')\n",
    "axes[0].set_xticklabels(['']+uniques.tolist(),rotation='vertical');\n",
    "axes[0].set_title('Data Features',y=1.2)\n",
    "axes[1].matshow(np.array(labels)[:,None], aspect='auto')\n",
    "axes[1].set_title('Data Labels',y=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fun with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus! Compute SURF features (requires openCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select some images and compute SURF. See that there's a parameter there. What is a reasonable one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: http://docs.opencv.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('/home/andreas/Andreas.jpg',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the surf object. The parameter is the threshold, roughly controlling the sensitivity to edges.\n",
    "surf = cv2.SURF(300)\n",
    "# Get the keypoints and descriptors for the image\n",
    "kp, des = surf.detectAndCompute(img,None)\n",
    "# Check how many keypoints were found\n",
    "print(len(kp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw the keypoints on top of the image\n",
    "img2 = cv2.drawKeypoints(img,kp,None,(255,0,0),4)\n",
    "plt.imshow(img2),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** TODO Q12 ***\n",
    "1. Play with the code above. Load any picture you want, and compute the SURF features. Adjust the threshold parameter to get more or less features.\n",
    "2. If you have more than one pictures, how can you extract consistent features across all pictures? *Hint*: Remember how we did that for text data. Here you might need to do an additional clustering step after you extract the \"words\" (in this case instead of words you have SURF descriptors), and now the histogram will say how many descriptors of each cluster we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've reached here and you still have time, try doing these exercises:\n",
    "1. Go back to the text processing section, and fit a classifier. See how the model performs and how you can make it better.\n",
    "2. Go back to the toy classification Q8 and argue about which feature combination is best by this time splitting the data into training and validation set to automatically select a good combination (e.g. search over a predifined selection of feature combinations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun with Images - This part is challenging, I leave it here if you want to have fun on your own time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If PIL is not installed, then only png can be read\n",
    "img = np.array(imread('PATH TO YOUR IMG')) \n",
    "\n",
    "# Lazy conversion from RGB to grayscale\n",
    "img = img[:,:,0]\n",
    "plt.imshow(img,cmap=plt.get_cmap('gray'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "imagePath = '' # HERE YOU PUT the path of the images folder, where you have your images stored. They have to be of consistent dimensions\n",
    "extensions = 'jpg' # Change if your images is something else\n",
    "\n",
    "listing = glob.glob(imagePath + '*.' + extensions)\n",
    "(height,width) = np.array(imread(listing[0],True))[:,:,0].shape\n",
    "print(height,width)\n",
    "images = np.array([imread(i,True)[:,:,0].flatten() for i in listing])\n",
    "\n",
    "plt.imshow(images[0,:].reshape(height,width), cmap=plt.get_cmap('gray'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute and plot the mean face\n",
    "mu = np.mean(images,0)\n",
    "plt.imshow(mu.reshape(height,width),cmap=plt.get_cmap('gray'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Center data\n",
    "images_cent = images-mu\n",
    "\n",
    "U,S,V = linalg.svd(images_cent.transpose(), full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigenfaces = U.T\n",
    "\n",
    "print(images.shape)\n",
    "print(eigenfaces.shape)\n",
    "\n",
    "# Each row of U is an eigen-face\n",
    "plt.imshow(eigenfaces[0,:].reshape(height,width),cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reconstruction\n",
    "weights = np.dot(images_cent, eigenfaces.T)\n",
    "print(weights.shape, eigenfaces.shape, images.shape)\n",
    "j=0\n",
    "\n",
    "img_j_reconstr = mu + np.dot(weights[j,:],eigenfaces)\n",
    "\n",
    "plt.imshow(img_j_reconstr.reshape(height,width),cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reconstruction by using fewer eigenfaces \n",
    "k=3\n",
    "img_j_reconstr_k = mu + np.dot(weights[j,0:k], eigenfaces[0:k])\n",
    "plt.imshow(img_j_reconstr_k.reshape(height,width),cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Credits: wellecks.wordpreess.com/tag/eigenfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Also try to do feature selection: Select only an area around the eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
