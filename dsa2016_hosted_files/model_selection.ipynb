{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning applications we seek to fit models that explain our data. From a family of models we try and determine the best model that explains our observations. Techniques of model selection offer principled approaches to determining the 'best' models.\n",
    "\n",
    "## Example - Linear regression\n",
    "\n",
    "Consider the regression problem where we observe an input variable $x$ and wish  predict a target variable $y$. The data are generated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "noise_std=.2 # Noise standard deviation\n",
    "num_points=20 # Number of data points\n",
    "X=np.linspace(0,1,num_points) #input variables\n",
    "Y=np.sin(2*np.pi*X)+noise_std*np.random.randn(len(X)) # target variables\n",
    "\n",
    "#Plot the points\n",
    "%matplotlib inline\n",
    "plt.plot(X,Y,'o',markersize=8)\n",
    "plt.xlabel(r'$x$',fontsize=20)\n",
    "plt.ylabel(r'$y$',fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we assume that we can use a polynomial to model the relationship between $x$ and $y$\n",
    "\\begin{eqnarray*}\n",
    "y&=&w_0+w_1x+w_2x^2+\\ldots+w_Px^P\\\\\n",
    "&=&\\sum_{i=0}^Pw_ix^i\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The model has a set of parameters $\\mathbf{w}=[w_0,\\ldots,w_P]^T$ and we can write \n",
    "\\begin{equation*}\n",
    "y=f(x,\\mathbf{w})\n",
    "\\end{equation*}\n",
    "Given a polynomial order $P$, we learn the parameters $\\mathbf{w}^\\ast$ that best explain the training data.\n",
    "\n",
    "\n",
    "By finding the $\\mathbf{w}^\\ast$ that minimizes the sum of square error\n",
    "\\begin{equation*}\n",
    "E(\\mathbf{w})=\\frac{1}{2}\\sum_{n=1}^N(f(x_n,\\mathbf{w})-y_n)^2\n",
    "\\end{equation*}\n",
    "we obtain the polynomial fit of the data $f(x,\\mathbf{w}^\\ast)$\n",
    "\n",
    "\n",
    "The numpy function $polyfit$ performs this minimization and returns the coefficients. The function $poly1d$ creates a polynomial object. (Use help to find out what these functions do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.polyfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.poly1d?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fiting the model\n",
    "\n",
    "We set the polynomial order and fit the polynomial via least squares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P=3\n",
    "polynomial=np.poly1d(np.polyfit(X, Y, P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the polynomial we create a grid and evaluate the polynomial at these points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_plot=np.linspace(0,1,100)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(X,Y,'o',markersize=8)\n",
    "plt.xlabel(r'$x$',fontsize=20)\n",
    "plt.ylabel(r'$y$',fontsize=20)\n",
    "plt.plot(x_plot,polynomial(x_plot),linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the model order and see what happens as you increase the order. What is the best value for $P$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Determining the ideal $P$ is problem in model selection. This parameter governs the complexity of the model.\n",
    "High values of $P$ are more flexible but harder to fit and may suffer numerical instability especially when data are limited.\n",
    "\n",
    "To determine a good value of $P$ we can monitor the error on a hold out test set. We divide the data into two, a training set and a test set. Here let us use half the data for training and the other half for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=X[range(0,len(X),2)]\n",
    "X_test=X[range(1,len(X),2)]\n",
    "Y_train=Y[range(0,len(Y),2)]\n",
    "Y_test=Y[range(1,len(Y),2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We compute the error on the hold out set for $P=0,1,\\ldots,9$, a total of 10 values. We also compute the error on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_order=10\n",
    "err_test=np.zeros(num_order)\n",
    "err_train=np.zeros(num_order)\n",
    "for order in range(num_order):\n",
    "    polynomial=np.poly1d(np.polyfit(X_train, Y_train, order))\n",
    "    err_test[order]=np.sqrt(sum((polynomial(X_test)-Y_test)**2)/len(X_test))\n",
    "    err_train[order]=np.sqrt(sum((polynomial(X_train)-Y_train)**2)/len(X_train))\n",
    "    \n",
    "%matplotlib inline    \n",
    "plt.plot(range(num_order),err_train,'-o',linewidth=2,markersize=10)\n",
    "plt.plot(range(num_order),err_test,'-o',linewidth=2,markersize=10)\n",
    "plt.legend(['Training data','Test data'])\n",
    "plt.xlabel('Polynomial Order')\n",
    "plt.ylabel('Error')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross-validation\n",
    "\n",
    "An alternative approach is to divide the data into $K$ (almost) equal sized parts and use $K-1$ parts to fit the model and test the model on the remaining part. We first dived the data using the scikit-learn package KFold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "import scipy.stats as ss #a package to compute statistics\n",
    "\n",
    "#Lets generate more data\n",
    "num_points=200 # Number of data points\n",
    "X=np.linspace(0,1,num_points) #input variables\n",
    "Y=np.sin(2*np.pi*X)+noise_std*np.random.randn(len(X)) # target variables\n",
    "\n",
    "num_order=10\n",
    "num_folds=5\n",
    "err_test=np.zeros((num_folds,num_order))\n",
    "kf = KFold(len(X), n_folds=num_folds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $kf$ now contains indices indicating which data points correspond to the training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for train_index, test_index in kf:\n",
    "    X_train=X[train_index]\n",
    "    X_test=X[test_index]\n",
    "    Y_train=Y[train_index]\n",
    "    Y_test=Y[test_index]\n",
    "    for order in range(num_order):\n",
    "        polynomial=np.poly1d(np.polyfit(X_train, Y_train, order))\n",
    "        err_test[i,order]=np.sqrt(sum((polynomial(X_test)-Y_test)**2)/len(X_test))\n",
    "        \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each polynomial order we have error rates for each of the K folds. We can compute the $95\\%$ confidence interval of the error and estimate model uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss.mstats.mquantiles(err_test)\n",
    "quantiles=ss.mstats.mquantiles(err_test,prob=[0.025, 0.5, 0.975],axis=0)\n",
    "\n",
    "%matplotlib inline  \n",
    "plt.errorbar(range(num_order), quantiles[1,:], yerr=[quantiles[0,:],quantiles[2,:]],elinewidth=2,linewidth=2)\n",
    "plt.xlabel('Polynomial Order')\n",
    "plt.ylabel('Error')\n",
    "plt.xlim([-1,10])\n",
    "plt.ylim([0,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Polynomial Regression\n",
    "\n",
    "We have a data set $D=\\{x_i,y_i\\}_{i=1}^N$ where $x_i$ are the input variables and $y_i$ are the target variable.\n",
    "We model the target variables as\n",
    "\\begin{equation}\n",
    "y_i=f(x_i,\\mathbf{w})+\\epsilon\n",
    "\\end{equation}\n",
    "Where $\\epsilon$ is zero mean Gaussian noise with variance $\\sigma_\\epsilon^2$\n",
    "and $\\mathbf{w}$ is a vector of the polynomial coefficients.\n",
    "\n",
    "The probability of the data $D$ given the parameters for a given model $\\mathcal{M}_i$ is\n",
    "\\begin{equation}\n",
    "p(D|\\mathbf{w},\\beta,\\mathcal{M}_i)=\\Big(\\frac{\\beta}{2\\pi}\\Big)^{N/2}\\exp\\Big(-\\frac{\\beta}{2}\\sum_{i=1}^N(f(x_i,\\mathbf{w})-y_i)^2\\Big)\n",
    "\\end{equation}\n",
    "where $\\beta=\\frac{1}{\\sigma_\\epsilon^2}$ is the noise precision\n",
    "\n",
    "### The Prior\n",
    "\n",
    "For a Bayesian treatment of the regression problem, we set a prior over the parameters $\\mathbf{w}$. This prior governs the types of interpolants we will obtain.\n",
    "If the magnitudes of the polynomial coefficients are restricted to small values, the model is inflexible and results in flat interpolants\n",
    "If the coefficients are allowed to be too large, then the model can be too flexible and oscillate wildly to pass all data points.\n",
    "\n",
    "We select the following prior\n",
    "\\begin{equation}\n",
    "p(\\mathbf{w}|\\alpha,\\mathcal{M}_i)=\\Big(\\frac{\\alpha}{2\\pi}\\Big)^{(P+1)/2}\\exp\\Big(-\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\Big)\n",
    "\\end{equation}\n",
    "where $\\alpha$ is the precision of the coefficients. $P$ is the polynomial order.\n",
    " When $\\alpha$ is small, coefficients can take large values\n",
    " When $\\alpha$ is large, coefficients are assumed to take small values.\n",
    " \n",
    " \n",
    " ### The Posterior\n",
    " \n",
    " The posterior distribution of $\\mathbf{w}$ is \n",
    "\\begin{equation}\n",
    "p(\\mathbf{w}|D,\\alpha,\\beta,\\mathcal{M}_i)=\\frac{p(D|\\mathbf{w},\\beta,\\mathcal{M}_i)p(\\mathbf{w}|\\alpha,\\mathcal{M}_i)}{p(D|\\alpha,\\beta,\\mathcal{M}_i)}\n",
    "\\end{equation}\n",
    " We can show that this is a Gaussian with \n",
    "\\begin{eqnarray*}\n",
    "\\mu&=&\\beta\\Sigma\\Phi\\mathbf{y}\\\\\n",
    "\\Sigma&=&[\\alpha\\mathbf{I}+\\beta\\Phi^T\\Phi]^{-1}\n",
    "\\end{eqnarray*}\n",
    "Where \n",
    "\\begin{equation}\n",
    "\\Phi=\\begin{bmatrix}\n",
    "    1 & x_{1} & x_{1}^2 & \\dots  & x_{1}^P \\\\\n",
    "    1 & x_{2} & x_{2}^2 & \\dots  & x_{2}^P \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    1 & x_{N} & x_{N}^2 & \\dots  & x_{N}^P\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Let us generate some data and see the effect of the prior on the inferred interpolant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha=10e-4\n",
    "noise_std=.2\n",
    "num_points=10\n",
    "X=np.linspace(0,1,num_points)\n",
    "Y=np.sin(2*np.pi*X)+noise_std*np.random.randn(len(X))\n",
    "\n",
    "P=9# assume a polynomial order\n",
    "\n",
    "Phi=np.array([X**i for i in range(P+1)])\n",
    "Phi=Phi.T\n",
    "beta=1./(noise_std*noise_std)\n",
    "A=beta*np.dot(Phi.T,Phi)+alpha*np.eye(P+1)\n",
    "Cov=np.linalg.inv(A)\n",
    "mu=beta*np.dot(np.dot(Cov,Phi.T),Y)\n",
    "\n",
    "#create a polynomial\n",
    "polynomial=np.poly1d(mu[::-1])\n",
    "x_plot=np.linspace(0,1,100)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(X,Y,'o',markersize=8)\n",
    "plt.plot(x_plot,polynomial(x_plot),linewidth=2)\n",
    "plt.xlabel(r'$x$',fontsize=20)\n",
    "plt.ylabel(r'$y$',fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the value of $\\alpha$ from very large to very small (say 100 to $10^{-4}$) and see the effect on the inferred interpolant. This parameter effectively controls the model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesian Information Criterion\n",
    "\n",
    "The Bayes factor comparing two models $i$ and $j$ is given by\n",
    "\\begin{equation}\n",
    "B_{ij}=\\frac{p(D|\\mathcal{M}_i)}{p(D|\\mathcal{M}_j)}\n",
    "\\end{equation}\n",
    "The log marginal likelihood $p(D|\\mathcal{M}_i)$ can be approximated using the Laplace approximation to yeild\n",
    "\\begin{equation}\n",
    "\\log p(D|\\mathcal{M}_i)\\approx \\log p(D|\\hat{\\theta}_{ML},\\mathcal{M}_i)-\\frac{d_i}{2}\\log(N)\n",
    "\\end{equation}\n",
    "where $\\hat{\\theta}_{ML}$, is the maximum likelihood parameter estimate and $d_i$ is the number of free parameters in model $\\mathcal{M}_i$\n",
    "\n",
    "The Bayesian Information Criterion for a model $\\mathcal{M}_i$ is defined as\n",
    "\\begin{equation}\n",
    "BIC_i=-2\\log p(D|\\hat{\\theta}_{ML},\\mathcal{M}_i)+d_i\\log(N)\n",
    "\\end{equation}\n",
    "\n",
    "1. The BIC statistic penalizes complex models\n",
    "2. It includes a penalty term that depends on the number of free parameters in a model\n",
    "3. We chose the model with the minimum BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define a function to compute the log likelihood given noise standard deviation\n",
    "#and ML estimate of the polynomial coefficients\n",
    "def loglik(x,y,f,sigma):\n",
    "    return -0.5*np.log(2*np.pi*sigma)-(1./(2*sigma**2))*sum((f(x)-y)**2)\n",
    "\n",
    "\n",
    "noise_std=.2 # Noise standard deviation\n",
    "num_points=20 # Number of data points\n",
    "X=np.linspace(0,1,num_points) #input variables\n",
    "Y=np.sin(2*np.pi*X)+noise_std*np.random.randn(len(X)) # target variables\n",
    "\n",
    "num_order=10\n",
    "BIC=np.zeros(num_order)\n",
    "for order in range(num_order):\n",
    "    polynomial=np.poly1d(np.polyfit(X, Y, order))\n",
    "    BIC[order]= -2*loglik(X,Y,polynomial,noise_std)+(order+1)*np.log(num_points)\n",
    "    \n",
    "%matplotlib inline \n",
    "plt.plot(range(num_order),BIC,linewidth=2)\n",
    "plt.xlabel('Polynomial Order')\n",
    "plt.ylabel('BIC')    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
